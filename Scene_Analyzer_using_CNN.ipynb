{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Scene Analyzer using CNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMefrVPCg-60"
      },
      "source": [
        "#Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzuOAJpxrCSo"
      },
      "source": [
        "import tensorflow as tf\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvE-heJNo3GG"
      },
      "source": [
        "# Preprocessing the Training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaRWYWK8r5_1",
        "outputId": "4ab66a74-3db6-408f-f02c-76ce6c80a290"
      },
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True)\n",
        "training_set = train_datagen.flow_from_directory(\n",
        "        'D:\\\\Scene Analyzer\\\\train',\n",
        "        target_size=(150, 150),\n",
        "        batch_size=32,\n",
        "        class_mode='categorical')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 14034 images belonging to 6 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrCMmGw9pHys"
      },
      "source": [
        "# Preprocessing the Test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifo1H7Dyt77E",
        "outputId": "43c68e81-b1b2-44c9-b0ff-269458beff15"
      },
      "source": [
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_set=test_datagen.flow_from_directory(\n",
        "        'D:\\\\Scene Analyzer\\\\test',\n",
        "        target_size=(150, 150),\n",
        "        batch_size=32,\n",
        "        class_mode='categorical')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 3000 images belonging to 6 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af8O4l90gk7B"
      },
      "source": [
        "#Building the CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6smv9cwuPbm"
      },
      "source": [
        "cnn=tf.keras.models.Sequential()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibiH9SH7ueDE"
      },
      "source": [
        "cnn.add(tf.keras.layers.Conv2D(filters=32,kernel_size=3,activation='relu',input_shape=[150,150,3]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pi05Jc1-vWc1"
      },
      "source": [
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2,strides=2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVUnXwyIwCT1"
      },
      "source": [
        "cnn.add(tf.keras.layers.Conv2D(filters=32,kernel_size=3,activation='relu'))\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2,strides=2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MagpHJEvwHQS"
      },
      "source": [
        "cnn.add(tf.keras.layers.Flatten())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mgs-c5udwVxa"
      },
      "source": [
        "cnn.add(tf.keras.layers.Dense(units=128,activation='relu'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLIftk7pwsX5"
      },
      "source": [
        "cnn.add(tf.keras.layers.Dense(units=6,activation='sigmoid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6XkI90snSDl"
      },
      "source": [
        "#Training the CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbONNJD1w3hH"
      },
      "source": [
        "cnn.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-iHfljuxB6i",
        "outputId": "837a59df-f37c-458d-f7c4-06512673901b"
      },
      "source": [
        "cnn.fit(x=training_set,validation_data=test_set,epochs=25)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "439/439 [==============================] - 319s 728ms/step - loss: 0.9725 - accuracy: 0.6196 - val_loss: 0.7499 - val_accuracy: 0.7220\n",
            "Epoch 2/25\n",
            "439/439 [==============================] - 302s 688ms/step - loss: 0.6759 - accuracy: 0.7525 - val_loss: 0.6207 - val_accuracy: 0.7860\n",
            "Epoch 3/25\n",
            "439/439 [==============================] - 301s 685ms/step - loss: 0.5902 - accuracy: 0.7862 - val_loss: 0.6311 - val_accuracy: 0.7797\n",
            "Epoch 4/25\n",
            "439/439 [==============================] - 301s 685ms/step - loss: 0.5300 - accuracy: 0.8118 - val_loss: 0.5363 - val_accuracy: 0.8113\n",
            "Epoch 5/25\n",
            "439/439 [==============================] - 300s 684ms/step - loss: 0.4847 - accuracy: 0.8246 - val_loss: 0.6276 - val_accuracy: 0.7810\n",
            "Epoch 6/25\n",
            "439/439 [==============================] - 300s 683ms/step - loss: 0.4352 - accuracy: 0.8422 - val_loss: 0.4872 - val_accuracy: 0.8370\n",
            "Epoch 7/25\n",
            "439/439 [==============================] - 301s 685ms/step - loss: 0.4157 - accuracy: 0.8506 - val_loss: 0.4892 - val_accuracy: 0.8307\n",
            "Epoch 8/25\n",
            "439/439 [==============================] - 300s 684ms/step - loss: 0.3888 - accuracy: 0.8584 - val_loss: 0.4595 - val_accuracy: 0.8383\n",
            "Epoch 9/25\n",
            "439/439 [==============================] - 301s 685ms/step - loss: 0.3647 - accuracy: 0.8657 - val_loss: 0.5609 - val_accuracy: 0.8167\n",
            "Epoch 10/25\n",
            "439/439 [==============================] - 301s 685ms/step - loss: 0.3391 - accuracy: 0.8774 - val_loss: 0.5962 - val_accuracy: 0.8143\n",
            "Epoch 11/25\n",
            "439/439 [==============================] - 302s 688ms/step - loss: 0.3389 - accuracy: 0.8769 - val_loss: 0.5780 - val_accuracy: 0.8230\n",
            "Epoch 12/25\n",
            "439/439 [==============================] - 302s 688ms/step - loss: 0.3027 - accuracy: 0.8901 - val_loss: 0.4926 - val_accuracy: 0.8320\n",
            "Epoch 13/25\n",
            "439/439 [==============================] - 299s 682ms/step - loss: 0.2934 - accuracy: 0.8942 - val_loss: 0.4893 - val_accuracy: 0.8447\n",
            "Epoch 14/25\n",
            "439/439 [==============================] - 299s 682ms/step - loss: 0.2726 - accuracy: 0.9009 - val_loss: 0.5389 - val_accuracy: 0.8347\n",
            "Epoch 15/25\n",
            "439/439 [==============================] - 299s 682ms/step - loss: 0.2676 - accuracy: 0.9007 - val_loss: 0.5284 - val_accuracy: 0.8380\n",
            "Epoch 16/25\n",
            "439/439 [==============================] - 299s 681ms/step - loss: 0.2434 - accuracy: 0.9111 - val_loss: 0.5676 - val_accuracy: 0.8210\n",
            "Epoch 17/25\n",
            "439/439 [==============================] - 299s 681ms/step - loss: 0.2418 - accuracy: 0.9112 - val_loss: 0.5656 - val_accuracy: 0.8270\n",
            "Epoch 18/25\n",
            "439/439 [==============================] - 299s 681ms/step - loss: 0.2344 - accuracy: 0.9153 - val_loss: 0.5802 - val_accuracy: 0.8320\n",
            "Epoch 19/25\n",
            "439/439 [==============================] - 299s 680ms/step - loss: 0.2315 - accuracy: 0.9140 - val_loss: 0.5507 - val_accuracy: 0.8393\n",
            "Epoch 20/25\n",
            "439/439 [==============================] - 299s 681ms/step - loss: 0.2147 - accuracy: 0.9230 - val_loss: 0.5342 - val_accuracy: 0.8450\n",
            "Epoch 21/25\n",
            "439/439 [==============================] - 299s 682ms/step - loss: 0.2067 - accuracy: 0.9266 - val_loss: 0.5871 - val_accuracy: 0.8277\n",
            "Epoch 22/25\n",
            "439/439 [==============================] - 299s 681ms/step - loss: 0.2049 - accuracy: 0.9267 - val_loss: 0.5966 - val_accuracy: 0.8393\n",
            "Epoch 23/25\n",
            "439/439 [==============================] - 300s 683ms/step - loss: 0.1863 - accuracy: 0.9290 - val_loss: 0.5812 - val_accuracy: 0.8350\n",
            "Epoch 24/25\n",
            "439/439 [==============================] - 300s 684ms/step - loss: 0.1916 - accuracy: 0.9313 - val_loss: 0.5817 - val_accuracy: 0.8420\n",
            "Epoch 25/25\n",
            "439/439 [==============================] - 299s 682ms/step - loss: 0.1819 - accuracy: 0.9333 - val_loss: 0.6440 - val_accuracy: 0.8397\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x1d495c0c490>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3PZasO0006Z"
      },
      "source": [
        "#Making a single prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acaUyZfVx1eC",
        "outputId": "8865fd54-7d2b-4313-9ee2-abebeb427319"
      },
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing import image\n",
        "path=''#input the test image path\n",
        "test_image=image.load_img(path,target_size=(150,150))\n",
        "test_image=tf.keras.preprocessing.image.img_to_array(test_image)\n",
        "test_image=np.expand_dims(test_image,axis=0)\n",
        "val=cnn.predict(test_image)\n",
        "# print(training_set.class_indices)\n",
        "print(val)\n",
        "if val[0][0]:\n",
        "  print('Buildings')\n",
        "elif val[0][1]:\n",
        "  print('Forest')\n",
        "elif val[0][2]:\n",
        "  print('Glacier')\n",
        "elif val[0][3]:\n",
        "  print('Mountain')\n",
        "elif val[0][4]:\n",
        "  print('Sea')\n",
        "else:\n",
        "  print('Street')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 1. 0. 0. 0.]]\n",
            "Glacier\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKPVWBCADsOB"
      },
      "source": [
        "#Saving Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vv4N7Vz4DUZd",
        "outputId": "b571083a-f756-4277-e729-1eb256e61861"
      },
      "source": [
        "cnn.save('D:\\Scene Analyzer')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: D:\\Scene Analyzer\\assets\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}